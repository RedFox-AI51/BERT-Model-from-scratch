# ğŸ§  BERT Model from Scratch

This project implements a BERT (Bidirectional Encoder Representations from Transformers) model **entirely from scratch** using PyTorch, including tokenization, model architecture, and training pipeline.

## ğŸš€ Features

- Custom tokenizer with vocabulary building
- BERT model implementation (multi-layer Transformer encoder)
- Masked Language Modeling (MLM) training
- Evaluation metrics & logging
- Configurable architecture via `TrainingConfig`

## ğŸ› ï¸ Requirements

Install dependencies:

```bash
pip install -r requirements.txt
```

## ğŸ§ª Training the Model

To start training:

```bash
python train_bert.py
```

Training settings can be modified in the script:
- `vocab_size`
- `hidden_size`
- `num_hidden_layers`
- `num_attention_heads`
- `intermediate_size`
- `max_position_embeddings`

## ğŸ“Š Example Output

Training logs include:
- Loss
- Learning rate
- Training accuracy
- Evaluation accuracy

Graph your logs using tools like Desmos, Matplotlib, or TensorBoard.

## ğŸ“ Project Structure

```
BERT-Model-from-scratch/
â”œâ”€â”€ tokenizer.py        # Custom tokenizer and vocab
â”œâ”€â”€ bert_model.py       # BERT model definition
â”œâ”€â”€ train_bert.py       # Training script
â”œâ”€â”€ quick_test.py       # Perform a quick test script
â”œâ”€â”€ test_bert.py        # Perform a regular test script
â”œâ”€â”€ utils.py            # Data prep, batching, and helpers
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md           # This file
```

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).

## ğŸ™‹â€â™‚ï¸ Author

**RedFox-AI51**  
If you like this project, feel free to â­ the repo or contribute!
